{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Stack V2 sample extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallel processing\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(progress_bar=True, nb_workers=16)\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME \"] = \"/home/hf_cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the random seed\n",
    "seed = 42\n",
    "\n",
    "#n.samples to be extracted from the final filtered samples\n",
    "n_samples = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk(\"/home/hf_cache/stackv2_java_content\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"bigcode/starcoder2-3b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "def tokenize_input_ptrain(batch):\n",
    "    return tokenizer(batch['content'], truncation = True, max_length=4096, padding = 'do_not_pad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "candidate_sample_size = 50000 \n",
    "sample_check_size = 100000 \n",
    "\n",
    "random_rows = np.random.choice(range(0, len(dataset)), size=candidate_sample_size + sample_check_size, replace=False)\n",
    "\n",
    "candidate_ds = dataset.select(random_rows[0:candidate_sample_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# candidates\n",
    "# I don't know why but after the tokenization I have a list of lists and I want to get rid of that\n",
    "candidate_ds = candidate_ds.map(tokenize_input_ptrain)\n",
    "df = candidate_ds.to_pandas()\n",
    "# filter out the input with a number of tokens > 300\n",
    "df['n_tok'] = df['input_ids'].apply(len)\n",
    "df = df[df['n_tok'] > 300 ]\n",
    "\n",
    "# save a copy for later\n",
    "df_query = df.copy()\n",
    "\n",
    "df_query['sample'] = df_query['input_ids'].progress_apply(lambda x: [x[i:i+300] for i in range(0, len(x)-299)])\n",
    "df_query = df_query.explode('sample') # explode the list of lists\n",
    "df_query['hash'] = df_query['sample'].progress_apply(lambda x: hash(tuple(x)))\n",
    "\n",
    "# count the hashes (to retrieve the one uniques)    \n",
    "hashes_ptrain = Counter(df_query['hash'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counter check 300 token sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_ds = dataset.select(random_rows[candidate_sample_size:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 10000\n",
    "for i in tqdm(range(0,len(check_ds), bs)):\n",
    "    df_batch = pd.DataFrame(check_ds[i:i+bs])\n",
    "    df_batch['input_ids'] = df_batch['content'].apply(lambda x: tokenizer(x, truncation = True, max_length=4096, padding = 'do_not_pad')['input_ids']) \n",
    "    df_batch['n_tok'] = df_batch['input_ids'].apply(len)\n",
    "    df_batch = df_batch[df_batch['n_tok'] > 300 ]\n",
    "    \n",
    "    df_batch['sample'] = df_batch['input_ids'].progress_apply(lambda x: [x[i:i+300] for i in range(0, len(x)-299)])\n",
    "    df_batch = df_batch.explode('sample') # explode the list of lists\n",
    "    df_batch['hash'] = df_batch['sample'].progress_apply(lambda x: hash(tuple(x)))\n",
    "    for h in df_batch['hash']:\n",
    "        if h in hashes_ptrain:\n",
    "            hashes_ptrain[h] += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the pre-train samples are not contained into the ftune set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#length of the tokenizer truncation used for the fine-tuning\n",
    "max_length = 1024\n",
    "\n",
    "def tokenize_input_ftune(batch):\n",
    "    #return tokenizer(batch['content'], return_tensors='pt')\n",
    "    return tokenizer(batch['content'], truncation = True, max_length= max_length, padding = 'do_not_pad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_dataset = load_dataset(\"../java_train\")\n",
    "tune_dataset = tune_dataset.remove_columns(['input_ids', 'attention_mask', 'n_tok', 'longline', 'alpha', 'encoded', 'autogen'])\n",
    "tune_dataset = tune_dataset.map(tokenize_input_ftune, batched=False, num_proc=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert it to a pandas dataframe\n",
    "tune_df = tune_dataset['train'].to_pandas()\n",
    "\n",
    "# filter out the input with a number of tokens > 300\n",
    "tune_df['n_tok'] = tune_df['input_ids'].apply(len)\n",
    "tune_df = tune_df[tune_df['n_tok'] > 300 ]\n",
    "\n",
    "# save a copy for later\n",
    "tune_df_query = tune_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_df_query['sample'] = tune_df_query['input_ids'].progress_apply(lambda x: [x[i:i+300] for i in range(0, len(x)-299)])\n",
    "tune_df_query = tune_df_query.explode('sample') # explode the list of lists\n",
    "tune_df_query['hash'] = tune_df_query['sample'].progress_apply(lambda x: hash(tuple(x)))\n",
    "\n",
    "# count the hashes (to retrieve the one uniques)\n",
    "hashes_ftune = Counter(tune_df_query['hash'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can filter out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the hash is 1 in the counter it means that it's unique. Zero as checked before in this case is not expected!\n",
    "df_query['uniques_1'] = df_query['hash'].progress_apply(lambda x: (hashes_ptrain[x] == 1) & (hashes_ftune[x] == 0))\n",
    "df_query['uniques_2'] = df_query['hash'].progress_apply(lambda x: (hashes_ptrain[x] == 2) & (hashes_ftune[x] == 0))\n",
    "df_query['uniques_3'] = df_query['hash'].progress_apply(lambda x: (hashes_ptrain[x] == 3) & (hashes_ftune[x] == 0))\n",
    "df_query['uniques_g3'] = df_query['hash'].progress_apply(lambda x: (hashes_ptrain[x] > 3) & (hashes_ftune[x] == 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I have to check if the prefix is unique, in such a way we will avoid false negatives!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the first 100 tokens of the prefix, that needs to be checked (250 to 300 suffix) (250 to 150) prefix to be checked\n",
    "df_query['sample_query'] = df_query['sample'].progress_apply(lambda x: x[150:250])\n",
    "\n",
    "# create the index\n",
    "df_index = df.copy()\n",
    "\n",
    "# generate a sliding window of 100 tokens from the training corpus\n",
    "df_index['patterns'] = df_index['input_ids'].progress_apply(lambda x: [x[i:i+100] for i in range(0, len(x)-99)])\n",
    "df_index = df_index.explode('patterns')\n",
    "\n",
    "# hash everything\n",
    "df_query['hash_sq'] = df_query['sample_query'].progress_apply(lambda x: hash(tuple(x)))\n",
    "df_index['hash'] = df_index['patterns'].progress_apply(lambda x: hash(tuple(x)))\n",
    "\n",
    "# count the hashes (to retrieve the one uniques)\n",
    "hashes_fn = Counter(df_index['hash'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## duplicates > 3\n",
    "\n",
    "The grater than 3 is a bit different, because we need to make sure that the hash in the counter matches the n. of duplicates.\n",
    "\n",
    "- df_sample_g3 -> 115439\n",
    "\n",
    "- df_sample_g3 prefix deduplicates -> 38605"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_g3 = df_query[df_query['uniques_g3']==True]\n",
    "print(df_sample_g3.shape)\n",
    "# we count the number of duplicates >3 but we want the actual number!!!\n",
    "df_sample_g3['n'] = df_sample_g3.groupby(by='hash')['hash'].transform('count')\n",
    "\n",
    "def condition(x):\n",
    "    return hashes_fn[x['hash_sq']] == x['n']\n",
    "\n",
    "# prefix duplicates\n",
    "df_sample_g3['uniques'] = df_sample_g3.progress_apply(condition, axis = 1)\n",
    "\n",
    "print(df_sample_g3[df_sample_g3['uniques']==True].shape)\n",
    "df_sampled_g3 = df_sample_g3[df_sample_g3['uniques']==True].drop_duplicates(subset=['hash']).sample(n = n_samples, replace = False, random_state=seed)\n",
    "\n",
    "df_sampled_g3['prefix_250'] = (df_sampled_g3['sample'].progress_apply(lambda x: tokenizer.decode(x[0:250], skip_special_tokens=False)))\n",
    "df_sampled_g3['prefix_200'] = (df_sampled_g3['sample'].progress_apply(lambda x: tokenizer.decode(x[50:250], skip_special_tokens=False)))\n",
    "df_sampled_g3['prefix_150'] = (df_sampled_g3['sample'].progress_apply(lambda x: tokenizer.decode(x[100:250], skip_special_tokens=False)))\n",
    "df_sampled_g3['prefix_100'] = (df_sampled_g3['sample'].progress_apply(lambda x: tokenizer.decode(x[150:250], skip_special_tokens=False)))\n",
    "\n",
    "df_sampled_g3['suffix'] = (df_sampled_g3['sample'].progress_apply(lambda x: tokenizer.decode(x[250:300], skip_special_tokens=False)))\n",
    "\n",
    "df_sampled_g3.to_parquet('mem-tune-replication_package/mem-tune/data/samples/pre-train/forget-attack/pre-train_attack_g3.parquet', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
