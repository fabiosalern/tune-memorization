{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a709040-4b44-4151-95ed-237377705466",
   "metadata": {},
   "source": [
    "# F-tune sampling\n",
    "\n",
    "- we control the number of times the sample was seen\n",
    "\n",
    "- additionally we are not considering prefix duplicates. The prefix deduplication was done for being able to do not have false negatives. Which means that for the same prefix we have different suffixes but in our sample set might be the case that we are not having that exact suffix (one of many for a single prefix) and we erroneously classify a sample as non memorized when actually it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da21b447-ab24-4480-b495-a44df612309c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, TextDataset, pipeline\n",
    "\n",
    "# parallel processing\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(progress_bar=True, nb_workers=16)\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# utility\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df672999-4dbf-4f57-9244-b72fdfe7c30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4adf3dd3-37cc-4444-a93b-37462443e132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the random seed\n",
    "seed = 42\n",
    "\n",
    "#length of the tokenizer truncation\n",
    "max_length = 1024\n",
    "\n",
    "#n.samples to be extracted from the final filtered samples\n",
    "n_samples = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01a65305-0976-40ac-8522-97f07ce8cac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer from HF-hub\n",
    "checkpoint = \"bigcode/starcoder2-3b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize_input_ftune(batch):\n",
    "    #return tokenizer(batch['content'], return_tensors='pt')\n",
    "    return tokenizer(batch['content'], truncation = True, max_length= max_length, padding = 'do_not_pad',  return_tensors='pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd54691e-6ba9-4531-9bb3-95a4528cfc59",
   "metadata": {},
   "source": [
    "# Create the fine-tune samples for the attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc653b2e-b5d7-4093-8876-cf7dd2596ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "dataset = dataset = load_dataset(\"parquet\", data_files={'train': './train_java.parquet'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9261de3-660e-4cd4-b0f4-84546cb87a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove non-necessery columns form the sample filtering\n",
    "dataset = dataset.remove_columns(['input_ids', 'attention_mask', 'n_tok', 'longline', 'alpha', 'encoded', 'autogen'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8932e9dd-f7e1-414b-88b6-e1ad91d6cb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the tokenizer\n",
    "dataset = dataset.map(tokenize_input_ftune, batched=False, num_proc=64)\n",
    "\n",
    "# convert it to a pandas dataframe\n",
    "df = dataset['train'].to_pandas()\n",
    "# I don't know why but after the tokenization I have a list of lists and I want to get rid of that\n",
    "df['input_ids'] = df['input_ids'].apply(lambda x: x[0])\n",
    "\n",
    "# filter out the input with a number of tokens > 300\n",
    "df['n_tok'] = df['input_ids'].apply(len)\n",
    "df = df[df['n_tok'] > 300 ]\n",
    "\n",
    "# save a copy for later\n",
    "df_query = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8949a2cc-79f9-4579-b4c0-4b3da5981135",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query['sample'] = df_query['input_ids'].progress_apply(lambda x: [x[i:i+300] for i in range(0, len(x)-299)])\n",
    "df_query = df_query.explode('sample') # explode the list of lists\n",
    "df_query['hash'] = df_query['sample'].progress_apply(lambda x: hash(tuple(x)))\n",
    "\n",
    "# count the hashes (to retrieve the one uniques)\n",
    "hashes_ptrain = Counter(df_query['hash'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b763a29c-480b-4c3b-a684-cda08744238e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the hash is 1 in the counter it means that it's unique. Zero as checked before in this case is not expected!\n",
    "df_query['uniques_1'] = df_query['hash'].progress_apply(lambda x: hashes_ptrain[x] == 1 )\n",
    "df_query['uniques_2'] = df_query['hash'].progress_apply(lambda x: hashes_ptrain[x] == 2 )\n",
    "df_query['uniques_3'] = df_query['hash'].progress_apply(lambda x: hashes_ptrain[x] == 3 )\n",
    "df_query['uniques_g3'] = df_query['hash'].progress_apply(lambda x: hashes_ptrain[x] > 3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b8a7c3-c922-4975-b683-efe7f12e40ba",
   "metadata": {},
   "source": [
    "Now I have to check if the prefix is unique, in such a way we will avoid false negatives!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd1f4b2-2cb4-4f6c-8781-264270b1d703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the first 100 tokens of the prefix, that needs to be checked (250 to 300 suffix) (250 to 150) prefix to be checked\n",
    "df_query['sample_query'] = df_query['sample'].progress_apply(lambda x: x[150:250])\n",
    "\n",
    "# create the index\n",
    "df_index = df.copy()\n",
    "\n",
    "# generate a sliding window of 100 tokens from the training corpus\n",
    "df_index['patterns'] = df_index['input_ids'].progress_apply(lambda x: [x[i:i+100] for i in range(0, len(x)-99)])\n",
    "df_index = df_index.explode('patterns')\n",
    "\n",
    "# hash everything\n",
    "df_query['hash_sq'] = df_query['sample_query'].progress_apply(lambda x: hash(tuple(x)))\n",
    "df_index['hash'] = df_index['patterns'].progress_apply(lambda x: hash(tuple(x)))\n",
    "\n",
    "# count the hashes (to retrieve the one uniques)\n",
    "hashes_ptrain = Counter(df_index['hash'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61f5d9e-7090-4887-9a3f-9156b2fd224e",
   "metadata": {},
   "source": [
    "Now that we can control the duplicated prefixes, we can perform the sampling considering different duplication sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b11d92-6048-42db-8ea3-27891332bffb",
   "metadata": {},
   "source": [
    "# No duplicates == 1\n",
    "\n",
    "- candidate samples -> 10060778\n",
    "- deduplicated prefix -> 9734999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7c5485-b0a7-4481-b6a7-42c16795e3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_1 = df_query[df_query['uniques_1']==True]\n",
    "print(df_sample_1.shape)\n",
    "# prefix check\n",
    "df_sample_1['uniques'] = df_sample_1['hash_sq'].progress_apply(lambda x: hashes_ptrain[x] == 1 )\n",
    "print(df_sample_1[df_sample_1['uniques']==True].shape)\n",
    "df_sampled_1 = df_sample_1[df_sample_1['uniques']==True].sample(n = n_samples, replace = False, random_state=seed)\n",
    "\n",
    "df_sampled_1['prefix_250'] = (df_sampled_1['sample'].progress_apply(lambda x: tokenizer.decode(x[0:250], skip_special_tokens=False)))\n",
    "df_sampled_1['prefix_200'] = (df_sampled_1['sample'].progress_apply(lambda x: tokenizer.decode(x[50:250], skip_special_tokens=False)))\n",
    "df_sampled_1['prefix_150'] = (df_sampled_1['sample'].progress_apply(lambda x: tokenizer.decode(x[100:250], skip_special_tokens=False)))\n",
    "df_sampled_1['prefix_100'] = (df_sampled_1['sample'].progress_apply(lambda x: tokenizer.decode(x[150:250], skip_special_tokens=False)))\n",
    "\n",
    "df_sampled_1['suffix'] = (df_sampled_1['sample'].progress_apply(lambda x: tokenizer.decode(x[250:300], skip_special_tokens=False)))\n",
    "\n",
    "df_sampled_1.to_parquet('/mem-tune/data/samples/memorization/fine-tune_attack_1.parquet', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab30048b-6853-42b2-b2f3-f32df3831356",
   "metadata": {},
   "source": [
    "## duplicates == 2; \n",
    "sample_2 -> 67056\n",
    "\n",
    "sample_2 deduplicated prefix -> **47940**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7529f5-15c4-434d-b277-d1dd01221f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_2 = df_query[df_query['uniques_2']==True]\n",
    "print(df_sample_2.shape)\n",
    "# prefix check\n",
    "df_sample_2['uniques'] = df_sample_2['hash_sq'].progress_apply(lambda x: hashes_ptrain[x] == 2 )\n",
    "\n",
    "print(df_sample_2[df_sample_2['uniques']==True].shape)\n",
    "\n",
    "df_sampled_2 = df_sample_2[df_sample_2['uniques']==True].drop_duplicates(subset=['hash']).sample(n = n_samples, replace = False, random_state=seed)\n",
    "\n",
    "df_sampled_2['prefix_250'] = (df_sampled_2['sample'].progress_apply(lambda x: tokenizer.decode(x[0:250], skip_special_tokens=False)))\n",
    "df_sampled_2['prefix_200'] = (df_sampled_2['sample'].progress_apply(lambda x: tokenizer.decode(x[50:250], skip_special_tokens=False)))\n",
    "df_sampled_2['prefix_150'] = (df_sampled_2['sample'].progress_apply(lambda x: tokenizer.decode(x[100:250], skip_special_tokens=False)))\n",
    "df_sampled_2['prefix_100'] = (df_sampled_2['sample'].progress_apply(lambda x: tokenizer.decode(x[150:250], skip_special_tokens=False)))\n",
    "\n",
    "df_sampled_2['suffix'] = (df_sampled_2['sample'].progress_apply(lambda x: tokenizer.decode(x[250:300], skip_special_tokens=False)))\n",
    "\n",
    "df_sampled_2.to_parquet('/mem-tune/data/samples/memorization/fine-tune_attack_2.parquet', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fa42ba-6e23-4d78-b210-38a4375065fd",
   "metadata": {},
   "source": [
    "## duplicates == 3\n",
    "\n",
    "- sample_3 -> 19209\n",
    "\n",
    "- sample_3 deduplicated prefix -> 10083"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81203973-f67d-4c71-97d7-90ba5fa69f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_3 = df_query[df_query['uniques_3']==True]\n",
    "print(df_sample_3.shape)\n",
    "# prefix check\n",
    "df_sample_3['uniques'] = df_sample_3['hash_sq'].progress_apply(lambda x: hashes_ptrain[x] == 3 )\n",
    "print(df_sample_3[df_sample_3['uniques']==True].shape)\n",
    "df_sampled_3 = df_sample_3[df_sample_3['uniques']==True].drop_duplicates(subset=['hash']).sample(n = n_samples, replace = False, random_state=seed)\n",
    "\n",
    "df_sampled_3['prefix_250'] = (df_sampled_3['sample'].progress_apply(lambda x: tokenizer.decode(x[0:250], skip_special_tokens=False)))\n",
    "df_sampled_3['prefix_200'] = (df_sampled_3['sample'].progress_apply(lambda x: tokenizer.decode(x[50:250], skip_special_tokens=False)))\n",
    "df_sampled_3['prefix_150'] = (df_sampled_3['sample'].progress_apply(lambda x: tokenizer.decode(x[100:250], skip_special_tokens=False)))\n",
    "df_sampled_3['prefix_100'] = (df_sampled_3['sample'].progress_apply(lambda x: tokenizer.decode(x[150:250], skip_special_tokens=False)))\n",
    "\n",
    "df_sampled_3['suffix'] = (df_sampled_3['sample'].progress_apply(lambda x: tokenizer.decode(x[250:300], skip_special_tokens=False)))\n",
    "\n",
    "df_sampled_3.to_parquet('/mem-tune/data/samples/memorization/fine-tune_attack_3.parquet', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79020df7-6d98-41ed-854d-d4d3e678c29d",
   "metadata": {},
   "source": [
    "## duplicates > 3\n",
    "\n",
    "The grater than 3 is a bit different, because we need to make sure that the hash in the counter matches the n. of duplicates.\n",
    "\n",
    "- df_sample_g3 -> 115439\n",
    "\n",
    "- df_sample_g3 prefix deduplicates -> 38605"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4b801a-7e7e-48e0-ba42-63d7016b4073",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_g3 = df_query[df_query['uniques_g3']==True]\n",
    "print(df_sample_g3.shape)\n",
    "# we count the number of duplicates >3 but we want the actual number!!!\n",
    "df_sample_g3['n'] = df_sample_g3.groupby(by='hash')['hash'].transform('count')\n",
    "\n",
    "def condition(x):\n",
    "    return hashes_ptrain[x['hash_sq']] == x['n']\n",
    "\n",
    "# prefix duplicates\n",
    "df_sample_g3['uniques'] = df_sample_g3.progress_apply(condition, axis = 1)\n",
    "\n",
    "print(df_sample_g3[df_sample_g3['uniques']==True].shape)\n",
    "df_sampled_g3 = df_sample_g3[df_sample_g3['uniques']==True].drop_duplicates(subset=['hash']).sample(n = n_samples, replace = False, random_state=seed)\n",
    "\n",
    "df_sampled_g3['prefix_250'] = (df_sampled_g3['sample'].progress_apply(lambda x: tokenizer.decode(x[0:250], skip_special_tokens=False)))\n",
    "df_sampled_g3['prefix_200'] = (df_sampled_g3['sample'].progress_apply(lambda x: tokenizer.decode(x[50:250], skip_special_tokens=False)))\n",
    "df_sampled_g3['prefix_150'] = (df_sampled_g3['sample'].progress_apply(lambda x: tokenizer.decode(x[100:250], skip_special_tokens=False)))\n",
    "df_sampled_g3['prefix_100'] = (df_sampled_g3['sample'].progress_apply(lambda x: tokenizer.decode(x[150:250], skip_special_tokens=False)))\n",
    "\n",
    "df_sampled_g3['suffix'] = (df_sampled_g3['sample'].progress_apply(lambda x: tokenizer.decode(x[250:300], skip_special_tokens=False)))\n",
    "\n",
    "df_sampled_g3.to_parquet('/mem-tune/data/samples/memorization/fine-tune_attack_g3.parquet', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
